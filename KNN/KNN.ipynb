{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import time\n",
    "import CQNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 卷积神经网络模型\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        # 编码器部分\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                1, 16, kernel_size=5, stride=1, padding=2\n",
    "            ),  # 使用不同的卷积核大小\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=7, stride=1, padding=3),  # 再增加一个并行层\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=9, stride=1, padding=4),  # 继续增加\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.encoder5 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=11, stride=1, padding=5),  # 再增加一个\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.encoder6 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=13, stride=1, padding=6),  # 最后一个并行层\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.encoder3_combined = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                16 * 6, 32, kernel_size=3, stride=1, padding=1\n",
    "            ),  # 更新为输入通道数为并行输出的总和\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        # 解码器部分\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 并行编码\n",
    "        encoded1 = self.encoder1(x)\n",
    "        encoded2 = self.encoder2(x)\n",
    "        encoded3 = self.encoder3(x)\n",
    "        encoded4 = self.encoder4(x)\n",
    "        encoded5 = self.encoder5(x)\n",
    "        encoded6 = self.encoder6(x)\n",
    "\n",
    "        # 将所有编码器的输出拼接在一起\n",
    "        encoded = torch.cat(\n",
    "            (encoded1, encoded2, encoded3, encoded4, encoded5, encoded6), dim=1\n",
    "        )  # 在通道维度拼接\n",
    "\n",
    "        # 继续编码\n",
    "        encoded = self.encoder3_combined(encoded)\n",
    "\n",
    "        # 解码过程\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一层并行\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 卷积神经网络模型\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # 编码器部分\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                1, 32, kernel_size=15, stride=1, padding=7\n",
    "            ),  # 使用不同的卷积核大小\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(\n",
    "                32, 64, kernel_size=3, stride=1, padding=1\n",
    "            ),  # 使用不同的卷积核大小\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        # self.encoder3 = nn.Sequential(\n",
    "        #     nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2, 2)\n",
    "        # )\n",
    "\n",
    "        # 解码器部分\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(192, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            # nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "            # nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 并行编码\n",
    "        encoded1 = self.encoder1(x)\n",
    "        encoded2 = self.encoder2(x)\n",
    "\n",
    "        # 将两个编码器的输出拼接在一起\n",
    "        encoded = torch.cat((encoded1, encoded2), dim=1)  # 在通道维度拼接\n",
    "\n",
    "        # 继续编码\n",
    "        # encoded = self.encoder3(encoded)\n",
    "\n",
    "        # 解码过程\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一层并行 & dropout\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # 编码器部分\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # nn.Dropout(0.25)  # 加入Dropout层，丢弃率为25%\n",
    "        )\n",
    "\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                1, 16, kernel_size=15, stride=1, padding=7\n",
    "            ),  # 使用不同的卷积核大小\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # nn.Dropout(0.25)  # 加入Dropout层，丢弃率为25%\n",
    "        )\n",
    "\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                1, 16, kernel_size=3, stride=1, padding=1\n",
    "            ),  # 使用不同的卷积核大小\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(4, 4),\n",
    "        )\n",
    "\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv2d(80, 80, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # nn.Dropout(0.25)  # 加入Dropout层\n",
    "        )\n",
    "\n",
    "        # 解码器部分\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(80, 32, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 并行编码\n",
    "        encoded1 = self.encoder1(x)\n",
    "        encoded2 = self.encoder2(x)\n",
    "        encoded3 = self.encoder3(x)\n",
    "\n",
    "        # 将两个编码器的输出拼接在一起\n",
    "        encoded = torch.cat((encoded1, encoded2, encoded3), dim=1)  # 在通道维度拼接\n",
    "\n",
    "        # 继续编码\n",
    "        encoded = self.encoder4(encoded)\n",
    "\n",
    "        # 解码过程\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 深度卷积神经网络模型\n",
    "class DeepConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepConvNet, self).__init__()\n",
    "        # 编码器部分\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self.encoder2 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        # 新增一个更深的编码器分支\n",
    "        self.encoder3 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=15, stride=1, padding=7),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "                # 新增一个更深的编码器分支\n",
    "        self.encoder4 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=25, stride=1, padding=12),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=7, stride=1, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        # 解码器部分\n",
    "        # 解码器部分，调整卷积转置层的参数来减少输出尺寸\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(384, 128, kernel_size=2, stride=2),  # 将通道从 384 减少到 128\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 1, kernel_size=2, stride=2),  # 减少通道数，增加空间分辨率\n",
    "            # nn.ReLU(),\n",
    "            # nn.BatchNorm2d(64),\n",
    "            # nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),  # 减少输出到目标大小\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 并行编码\n",
    "        encoded1 = self.encoder1(x)\n",
    "        encoded2 = self.encoder2(x)\n",
    "        encoded3 = self.encoder3(x)\n",
    "        encoded4 = self.encoder4(x)\n",
    "\n",
    "        # 将三个编码器的输出拼接在一起\n",
    "        encoded = torch.cat((encoded1, encoded2, encoded3, encoded4), dim=1)  # 在通道维度拼接\n",
    "\n",
    "        # 解码过程\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origin model\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # 编码器部分\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        # 解码器部分\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1, 600, 512])\n"
     ]
    }
   ],
   "source": [
    "train_num = 40\n",
    "# Read and reshape image data\n",
    "\n",
    "file_path = \"./data/go22-26m.dat\"\n",
    "\n",
    "img_8bit_matrix = CQNet.read_sonar_data(file_path, train_num)\n",
    "\n",
    "ans = CQNet.reshape_img_matrix(img_8bit_matrix, train_num)  # ans为最后读出来的三维数组\n",
    "\n",
    "ans_tensor, labels_tensor = CQNet.prepare_data_and_labels(\n",
    "    ans, train_num, show_labels=False, weather_plot=False\n",
    ")\n",
    "# new_ans_tensor, new_labels_tensor = prepare_data_and_labels(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化卷积网络\n",
    "conv_net = DeepConvNet()\n",
    "# 检查 GPU 是否可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# 加载预训练模型的权重\n",
    "conv_net.load_state_dict(torch.load(\"./model/SuperDeep.pth\"))\n",
    "# 将模型移动到 GPU\n",
    "conv_net.to(device)\n",
    "# 将数据和标签移动到 GPU\n",
    "ans_tensor = ans_tensor.to(device)\n",
    "labels_tensor = labels_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training supervised model...\n",
      "Epoch [1/6], Loss: 0.0165, Time: 289.75607895851135\n",
      "Epoch [2/6], Loss: 0.0137, Time: 285.9897840023041\n",
      "Epoch [3/6], Loss: 0.0086, Time: 314.0784549713135\n",
      "Epoch [4/6], Loss: 0.0065, Time: 346.04535484313965\n",
      "Epoch [5/6], Loss: 0.0059, Time: 347.8063986301422\n",
      "Epoch [6/6], Loss: 0.0042, Time: 373.76744556427\n"
     ]
    }
   ],
   "source": [
    "# for i in range(20):\n",
    "# 开始训练卷积神经网络\n",
    "CQNet.train_supervised_model(conv_net, ans_tensor, train_num = 6, labels= labels_tensor, batch_size=8, learning_rate=0.005)\n",
    "# 保存模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "torch.save(conv_net.state_dict(), \"./model/SuperDeep.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用训练后的模型进行预测\n",
    "conv_net.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = conv_net(ans_tensor)\n",
    "\n",
    "# 交互式显示切片和预测结果\n",
    "CQNet.interactive_plot(ans_tensor[0:1], predictions[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_tensor[0:10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# 创建模型实例\n",
    "conv_net = ConvNet()\n",
    "\n",
    "# 生成一个随机输入数据\n",
    "x = torch.randn(1, 1, 1200, 1024)  # 根据您的输入尺寸进行调整\n",
    "\n",
    "# 前向传播\n",
    "y = conv_net(x)\n",
    "\n",
    "# 可视化计算图\n",
    "dot = make_dot(y, params=dict(conv_net.named_parameters()))\n",
    "dot.render(\"conv_net_architecture\", format=\"png\")  # 生成 PNG 文件"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
